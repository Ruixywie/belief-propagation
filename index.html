<!DOCTYPE html>
<html lang="zh-CN">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Belief Propagation Report / ä¿¡å¿µä¼ æ’­æŠ¥å‘Š</title>

<!-- KaTeX -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false}
    ],
    throwOnError: false
  });"></script>

<style>
/* â”€â”€ Base â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
:root {
  --bg: #ffffff;
  --fg: #1a1a2e;
  --accent: #2563eb;
  --accent-hover: #1d4ed8;
  --code-bg: #f3f4f6;
  --border: #e5e7eb;
  --caption: #4b5563;
}
*, *::before, *::after { box-sizing: border-box; }
body {
  font-family: "Noto Sans SC", "Segoe UI", system-ui, sans-serif;
  line-height: 1.8;
  color: var(--fg);
  background: var(--bg);
  max-width: 900px;
  margin: 0 auto;
  padding: 2rem 1.5rem 4rem;
}

/* â”€â”€ Headings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
h1 { font-size: 2rem; border-bottom: 3px solid var(--accent); padding-bottom: .4em; }
h2 { font-size: 1.5rem; margin-top: 2.5rem; border-bottom: 1px solid var(--border); padding-bottom: .3em; }
h3 { font-size: 1.25rem; margin-top: 2rem; }
h4 { font-size: 1.1rem; margin-top: 1.5rem; }

/* â”€â”€ Links â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
a { color: var(--accent); text-decoration: none; }
a:hover { text-decoration: underline; }

/* â”€â”€ Images â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.image-container {
  text-align: center;
  margin: 1.5rem 0;
}
.image-container img {
  max-width: 100%;
  height: auto;
  border-radius: 6px;
  box-shadow: 0 2px 8px rgba(0,0,0,.1);
}

/* â”€â”€ Videos â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.video-container {
  text-align: center;
  margin: 2rem 0;
  background: #f9fafb;
  border-radius: 8px;
  padding: 1rem;
}
.video-container video {
  max-width: 100%;
  border-radius: 6px;
}
.video-caption {
  font-weight: 600;
  color: var(--caption);
  margin-bottom: .5rem;
}

/* â”€â”€ Download link â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.download-link {
  display: inline-block;
  margin-top: .5rem;
  padding: .35rem 1rem;
  font-size: .875rem;
  color: var(--accent);
  border: 1px solid var(--accent);
  border-radius: 4px;
  transition: background .2s, color .2s;
}
.download-link:hover {
  background: var(--accent);
  color: #fff;
  text-decoration: none;
}

/* â”€â”€ Code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
code {
  font-family: "Fira Code", "Cascadia Code", monospace;
  font-size: .9em;
  background: var(--code-bg);
  padding: .15em .35em;
  border-radius: 3px;
}
pre {
  background: var(--code-bg);
  padding: 1rem;
  border-radius: 6px;
  overflow-x: auto;
}
pre code { background: none; padding: 0; }

/* â”€â”€ Tables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
table {
  width: 100%;
  border-collapse: collapse;
  margin: 1.5rem 0;
  font-size: .95rem;
}
th, td {
  border: 1px solid var(--border);
  padding: .5rem .75rem;
  text-align: left;
}
th { background: var(--code-bg); font-weight: 600; }

/* â”€â”€ Blockquotes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
blockquote {
  border-left: 4px solid var(--accent);
  margin: 1rem 0;
  padding: .5rem 1rem;
  color: var(--caption);
  background: #f0f7ff;
  border-radius: 0 6px 6px 0;
}

/* â”€â”€ Lists â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
ul, ol { padding-left: 1.5rem; }
li { margin-bottom: .3rem; }

/* â”€â”€ Horizontal rule â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
hr { border: none; border-top: 1px solid var(--border); margin: 2rem 0; }

/* â”€â”€ KaTeX overrides â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
.katex-display { overflow-x: auto; overflow-y: hidden; padding: .5rem 0; }

/* â”€â”€ Responsive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ */
@media (max-width: 600px) {
  body { padding: 1rem; }
  h1 { font-size: 1.5rem; }
  h2 { font-size: 1.25rem; }
  table { font-size: .85rem; }
}
</style>
</head>
<body>
<h1>Belief Propagation: Principles, Algorithms, and Applications</h1>
<p>ä¿¡å¿µä¼ æ’­ï¼šåŸç†ã€ç®—æ³•å’Œåº”ç”¨</p>
<hr />
<h2>Table of Contents</h2>
<p>ç›®å½•</p>
<ol>
<li><a href="#1-introduction">Introduction
ä»‹ç»</a></li>
<li><a href="#2-probabilistic-graphical-models">Probabilistic Graphical Models
æ¦‚ç‡å›¾æ¨¡å‹</a></li>
<li><a href="#3-factor-graphs">Factor Graphs
å› å­å›¾</a></li>
<li><a href="#4-message-passing">Message Passing
æ¶ˆæ¯ä¼ é€’</a></li>
<li><a href="#5-the-sum-product-algorithm">The Sum-Product Algorithm
å’Œç§¯ç®—æ³•</a></li>
<li><a href="#6-the-max-product-algorithm">The Max-Product Algorithm
æœ€å¤§ä¹˜ç§¯ç®—æ³•</a></li>
<li><a href="#7-exact-inference-on-trees">Exact Inference on Trees
æ ‘ä¸Šçš„ç²¾ç¡®æ¨ç†</a></li>
<li><a href="#8-loopy-belief-propagation">Loopy Belief Propagation
å¾ªç¯ä¿¡å¿µä¼ æ’­</a></li>
<li><a href="#9-numerical-example">Numerical Example
æ•°å€¼ç¤ºä¾‹</a></li>
<li><a href="#10-applications">Applications
åº”ç”¨ç¨‹åº</a></li>
<li><a href="#11-conclusion">Conclusion
ç»“è®º</a></li>
<li><a href="#12-references">References
å‚è€ƒ</a></li>
</ol>
<hr />
<h2>1. Introduction</h2>
<p>1. å¼•è¨€</p>
<p>Probabilistic inference is a cornerstone of modern machine learning, statistics, and artificial intelligence. Given a joint probability distribution over a set of random variables, we often wish to answer queries such as:
æ¦‚ç‡æ¨ç†æ˜¯ç°ä»£æœºå™¨å­¦ä¹ ã€ç»Ÿè®¡å­¦å’Œäººå·¥æ™ºèƒ½çš„åŸºçŸ³ã€‚ç»™å®šä¸€ç»„éšæœºå˜é‡çš„è”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œæˆ‘ä»¬å¸¸å¸¸å¸Œæœ›å›ç­”å¦‚ä¸‹é—®é¢˜ï¼š</p>
<ul>
<li><strong>Marginal inference</strong>: What is the probability of a single variable $x_i$ taking a particular value, after summing out all other variables?
<strong>è¾¹é™…æ¨æ–­</strong> ï¼šå•ä¸ªå˜é‡ x çš„æ¦‚ç‡æ˜¯å¤šå°‘ï¼Ÿ ğ‘– x i â€‹ åœ¨å°†æ‰€æœ‰å…¶ä»–å˜é‡ç›¸åŠ ä¹‹åï¼Œå–æŸä¸ªç‰¹å®šå€¼ï¼Ÿ</li>
<li><strong>MAP inference</strong>: What is the most probable joint assignment to all variables?
<strong>æœ€å¤§åéªŒæ¦‚ç‡ (MAP) æ¨æ–­</strong> ï¼šæ‰€æœ‰å˜é‡æœ€å¯èƒ½çš„è”åˆèµ‹å€¼æ˜¯ä»€ä¹ˆï¼Ÿ</li>
<li><strong>Conditional inference</strong>: What is the posterior distribution of some variables given observed evidence?
<strong>æ¡ä»¶æ¨æ–­</strong> ï¼šç»™å®šè§‚æµ‹è¯æ®ï¼ŒæŸäº›å˜é‡çš„åéªŒåˆ†å¸ƒæ˜¯ä»€ä¹ˆï¼Ÿ</li>
</ul>
<p>For models with many variables, exact computation of these quantities involves summing or maximizing over an exponentially large state space, making brute-force enumeration intractable. <strong>Belief Propagation (BP)</strong> provides an elegant and efficient framework for performing these computations by exploiting the structure of the underlying probabilistic graphical model.
å¯¹äºå…·æœ‰ä¼—å¤šå˜é‡çš„æ¨¡å‹ï¼Œç²¾ç¡®è®¡ç®—è¿™äº›é‡éœ€è¦åœ¨æŒ‡æ•°çº§åºå¤§çš„çŠ¶æ€ç©ºé—´ä¸Šè¿›è¡Œæ±‚å’Œæˆ–æœ€å¤§åŒ–ï¼Œè¿™ä½¿å¾—ç©·ä¸¾æ³•éš¾ä»¥å®ç°ã€‚ <strong>ç½®ä¿¡ä¼ æ’­ï¼ˆBPï¼‰</strong> é€šè¿‡åˆ©ç”¨åº•å±‚æ¦‚ç‡å›¾æ¨¡å‹çš„ç»“æ„ï¼Œæä¾›äº†ä¸€ä¸ªä¼˜é›…è€Œé«˜æ•ˆçš„æ¡†æ¶æ¥æ‰§è¡Œè¿™äº›è®¡ç®—ã€‚</p>
<p>Originally introduced by Judea Pearl in 1982 for tree-structured models, Belief Propagation is a <strong>message-passing algorithm</strong> that computes exact marginals on trees and provides powerful approximations on general graphs. It has become one of the most widely used algorithms in probabilistic reasoning, with applications spanning error-correcting codes, computer vision, natural language processing, and computational biology.
ä¿¡å¿µä¼ æ’­ç®—æ³•æœ€åˆç”± Judea Pearl äº 1982 å¹´æå‡ºï¼Œç”¨äºæ ‘çŠ¶ç»“æ„æ¨¡å‹ã€‚å®ƒæ˜¯ä¸€ç§<strong>æ¶ˆæ¯ä¼ é€’ç®—æ³•</strong> ï¼Œèƒ½å¤Ÿè®¡ç®—æ ‘ä¸Šçš„ç²¾ç¡®è¾¹ç¼˜åˆ†å¸ƒï¼Œå¹¶ä¸ºä¸€èˆ¬å›¾æä¾›å¼ºå¤§çš„è¿‘ä¼¼è§£ã€‚å¦‚ä»Šï¼Œå®ƒå·²æˆä¸ºæ¦‚ç‡æ¨ç†é¢†åŸŸåº”ç”¨æœ€å¹¿æ³›çš„ç®—æ³•ä¹‹ä¸€ï¼Œå…¶åº”ç”¨æ¶µç›–çº é”™ç ã€è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†å’Œè®¡ç®—ç”Ÿç‰©å­¦ç­‰é¢†åŸŸã€‚</p>
<p>This report provides a comprehensive introduction to Belief Propagation, starting from the foundations of probabilistic graphical models and building up to the Sum-Product and Max-Product algorithms, with both theoretical exposition and visual illustrations.
æœ¬æŠ¥å‘Šå…¨é¢ä»‹ç»äº†ä¿¡å¿µä¼ æ’­ï¼Œä»æ¦‚ç‡å›¾æ¨¡å‹çš„åŸºç¡€çŸ¥è¯†å…¥æ‰‹ï¼Œé€æ­¥æ·±å…¥åˆ°æ±‚å’Œç§¯ç®—æ³•å’Œæœ€å¤§ç§¯ç®—æ³•ï¼Œå¹¶è¾…ä»¥ç†è®ºé˜è¿°å’Œè§†è§‰ç¤ºä¾‹ã€‚</p>
<hr />
<h2>2. Probabilistic Graphical Models</h2>
<p>2. æ¦‚ç‡å›¾æ¨¡å‹</p>
<p>A <strong>Probabilistic Graphical Model (PGM)</strong> represents a joint probability distribution using a graph, where nodes correspond to random variables and edges encode conditional dependencies or interactions. PGMs come in three main flavors:
<strong>æ¦‚ç‡å›¾æ¨¡å‹ (PGM)</strong> ä½¿ç”¨å›¾æ¥è¡¨ç¤ºè”åˆæ¦‚ç‡åˆ†å¸ƒï¼Œå…¶ä¸­èŠ‚ç‚¹å¯¹åº”äºéšæœºå˜é‡ï¼Œè¾¹ç¼–ç æ¡ä»¶ä¾èµ–å…³ç³»æˆ–äº¤äº’ä½œç”¨ã€‚PGM ä¸»è¦æœ‰ä¸‰ç§ç±»å‹ï¼š</p>
<p><div class="image-container"><img src="media/images/bp_scenes/PGMOverview_ManimCE_v0.19.2.png" alt="Probabilistic Graphical Models Overview"><a class="download-link" href="media/images/bp_scenes/PGMOverview_ManimCE_v0.19.2.png" download>â¬‡ ä¸‹è½½å›¾ç‰‡</a></div></p>
<h3>2.1 Bayesian Networks (Directed Models)</h3>
<p>2.1 è´å¶æ–¯ç½‘ç»œï¼ˆæœ‰å‘æ¨¡å‹ï¼‰</p>
<p>A <strong>Bayesian Network</strong> (BN) is a directed acyclic graph (DAG) where each node $x_i$ is associated with a conditional probability distribution given its parents:
<strong>è´å¶æ–¯ç½‘ç»œ</strong> ï¼ˆBNï¼‰æ˜¯ä¸€ä¸ªæœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ï¼Œå…¶ä¸­æ¯ä¸ªèŠ‚ç‚¹ğ‘¥ ğ‘– x i â€‹ ä¸ç»™å®šå…¶çˆ¶æ¯çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒç›¸å…³ï¼š</p>
<p>$p(x_1, x_2, \ldots, x_n) = \prod_{i=1}^{n} p(x_i \mid \text{pa}(x_i))$</p>
<p>The directed edges encode causal or generative relationships. For example, in a medical diagnosis model, a disease node might point to symptom nodes, representing that the disease <em>causes</em> the symptoms.
æœ‰å‘è¾¹ç¼–ç å› æœå…³ç³»æˆ–ç”Ÿæˆå…³ç³»ã€‚ä¾‹å¦‚ï¼Œåœ¨åŒ»å­¦è¯Šæ–­æ¨¡å‹ä¸­ï¼Œç–¾ç—…èŠ‚ç‚¹å¯èƒ½æŒ‡å‘ç—‡çŠ¶èŠ‚ç‚¹ï¼Œè¡¨ç¤ºè¯¥ç–¾ç—…<em>å¯¼è‡´äº†</em>è¿™äº›ç—‡çŠ¶ã€‚</p>
<p><strong>Key properties:
ä¸»è¦ç‰¹æ€§ï¼š</strong></p>
<ul>
<li>Encodes conditional independencies via the <em>d-separation</em> criterion
é€šè¿‡ <em>d åˆ†ç¦»</em>å‡†åˆ™å¯¹æ¡ä»¶ç‹¬ç«‹æ€§è¿›è¡Œç¼–ç </li>
<li>Naturally represents causal/generative processes
è‡ªç„¶åœ°ä»£è¡¨å› æœ/ç”Ÿæˆè¿‡ç¨‹</li>
<li>Parameters are conditional probability tables (CPTs)
å‚æ•°æ˜¯æ¡ä»¶æ¦‚ç‡è¡¨ï¼ˆCPTï¼‰ã€‚</li>
</ul>
<h3>2.2 Markov Random Fields (Undirected Models)</h3>
<p>2.2 é©¬å°”å¯å¤«éšæœºåœºï¼ˆæ— å‘æ¨¡å‹ï¼‰</p>
<p>A <strong>Markov Random Field</strong> (MRF), also called an undirected graphical model, uses an undirected graph where the joint distribution factorizes over cliques:
<strong>é©¬å°”å¯å¤«éšæœºåœº</strong> ï¼ˆMRFï¼‰ï¼Œä¹Ÿç§°ä¸ºæ— å‘å›¾æ¨¡å‹ï¼Œä½¿ç”¨æ— å‘å›¾ï¼Œå…¶ä¸­è”åˆåˆ†å¸ƒåœ¨å›¢ä¸Šåˆ†è§£ï¼š</p>
<p>$p(x_1, x_2, \ldots, x_n) = \frac{1}{Z} \prod_{c \in \mathcal{C}} \psi_c(\mathbf{x}_c)$</p>
<p>where $\psi_c$ are non-negative <strong>potential functions</strong> defined over cliques $c$, and $Z = \sum_{\mathbf{x}} \prod_c \psi_c(\mathbf{x}<em>c)$ is the <strong>partition function</strong> ensuring normalization.
å…¶ä¸­ ğœ“ ğ‘ Ïˆ c â€‹ æ˜¯å®šä¹‰åœ¨å›¢ $c$ ä¸Šçš„éè´Ÿ<strong>åŠ¿å‡½æ•°</strong> ï¼Œè€Œ $Z = \sum</em>{\mathbf{x}} \prod_c \psi_c(\mathbf{x}_c)$ æ˜¯ç¡®ä¿å½’ä¸€åŒ–çš„<strong>é…åˆ†å‡½æ•°</strong> ã€‚</p>
<p><strong>Key properties:
ä¸»è¦ç‰¹æ€§ï¼š</strong></p>
<ul>
<li>Encodes symmetric relationships (no notion of directionality)
ç¼–ç å¯¹ç§°å…³ç³»ï¼ˆæ— æ–¹å‘æ€§æ¦‚å¿µï¼‰</li>
<li>Conditional independencies follow from graph separation
æ¡ä»¶ç‹¬ç«‹æ€§æºäºå›¾åˆ†ç¦»</li>
<li>Widely used in image processing and spatial statistics (e.g., Ising model)
å¹¿æ³›åº”ç”¨äºå›¾åƒå¤„ç†å’Œç©ºé—´ç»Ÿè®¡ï¼ˆä¾‹å¦‚ï¼Œä¼Šè¾›æ¨¡å‹ï¼‰</li>
</ul>
<h3>2.3 Factor Graphs (Bipartite Representation)</h3>
<p>2.3 å› å­å›¾ï¼ˆäºŒåˆ†å›¾è¡¨ç¤ºï¼‰</p>
<p>A <strong>Factor Graph</strong> is a bipartite graph with two types of nodes â€” <strong>variable nodes</strong> and <strong>factor nodes</strong> â€” connected by edges. It provides a unified and more fine-grained representation that can encode both directed and undirected models. Factor graphs are the natural setting for Belief Propagation, and we discuss them in detail in the next section.
<strong>å› å­å›¾</strong>æ˜¯ä¸€ç§äºŒåˆ†å›¾ï¼Œå®ƒç”±ä¸¤ç§ç±»å‹çš„èŠ‚ç‚¹â€”â€” <strong>å˜é‡èŠ‚ç‚¹</strong>å’Œ<strong>å› å­èŠ‚ç‚¹</strong> â€”â€”é€šè¿‡è¾¹è¿æ¥è€Œæˆã€‚å®ƒæä¾›äº†ä¸€ç§ç»Ÿä¸€ä¸”æ›´ç»†ç²’åº¦çš„è¡¨ç¤ºæ–¹æ³•ï¼Œå¯ä»¥ç¼–ç æœ‰å‘æ¨¡å‹å’Œæ— å‘æ¨¡å‹ã€‚å› å­å›¾æ˜¯ç½®ä¿¡ä¼ æ’­çš„å¤©ç„¶æ¡†æ¶ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚è¯¦ç»†è®¨è®ºã€‚</p>
<hr />
<h2>3. Factor Graphs</h2>
<p>3. å› å­å›¾</p>
<p>A factor graph makes the factorization of a probability distribution explicit. Given a joint distribution that factorizes as:
å› å­å›¾å¯ä»¥æ˜ç¡®åœ°è¡¨ç¤ºæ¦‚ç‡åˆ†å¸ƒçš„å› å­åˆ†è§£ã€‚ç»™å®šä¸€ä¸ªå¯ä»¥åˆ†è§£ä¸ºä»¥ä¸‹å½¢å¼çš„è”åˆåˆ†å¸ƒï¼š</p>
<p>$p(x_1, x_2, \ldots, x_n) = \prod_{a} f_a(\mathbf{x}_a)$</p>
<p>where each $f_a$ is a <strong>factor</strong> (a non-negative function) that depends on a subset $\mathbf{x}_a$ of variables, the factor graph is constructed as follows:
å…¶ä¸­æ¯ä¸ª ğ‘“ ğ‘ f a â€‹ æ˜¯ä¸€ä¸ª<strong>å› å­</strong> ï¼ˆä¸€ä¸ªéè´Ÿå‡½æ•°ï¼‰ï¼Œå®ƒä¾èµ–äºå­é›†ğ‘¥ ğ‘ x a â€‹ å¯¹äºæ‰€æœ‰å˜é‡ï¼Œå› å­å›¾çš„æ„å»ºæ–¹å¼å¦‚ä¸‹ï¼š</p>
<ul>
<li><strong>Variable nodes</strong> (circles): One for each random variable $x_i$
<strong>å˜é‡èŠ‚ç‚¹</strong> ï¼ˆåœ†åœˆï¼‰ï¼šæ¯ä¸ªéšæœºå˜é‡ $x_i$ å¯¹åº”ä¸€ä¸ªèŠ‚ç‚¹ã€‚</li>
<li><strong>Factor nodes</strong> (squares): One for each factor $f_a$
<strong>å› å­èŠ‚ç‚¹</strong> ï¼ˆæ–¹æ ¼ï¼‰ï¼šæ¯ä¸ªå› å­å¯¹åº”ä¸€ä¸ª $f_a$</li>
<li><strong>Edges</strong>: An edge connects variable node $x_i$ to factor node $f_a$ if and only if $x_i \in \mathbf{x}_a$
<strong>è¾¹</strong> ï¼šè¾¹è¿æ¥å˜é‡èŠ‚ç‚¹ğ‘¥ ğ‘– x i â€‹ å¯¹èŠ‚ç‚¹ ğ‘“ è¿›è¡Œå› å­åˆ†æ ğ‘ f a â€‹ å½“ä¸”ä»…å½“ $x_i \in \mathbf{x}_a$</li>
</ul>
<p><div class="image-container"><img src="media/images/bp_scenes/FactorGraphIntro_ManimCE_v0.19.2.png" alt="Factor Graph Structure"><a class="download-link" href="media/images/bp_scenes/FactorGraphIntro_ManimCE_v0.19.2.png" download>â¬‡ ä¸‹è½½å›¾ç‰‡</a></div></p>
<p>In the figure above, the joint distribution $p(x_1, x_2, x_3, x_4) = f_a(x_1, x_2) \cdot f_b(x_2, x_3) \cdot f_c(x_3, x_4)$ is represented by a chain-like factor graph with variable nodes (blue circles) and factor nodes (red squares).
åœ¨ä¸Šå›¾ä¸­ï¼Œè”åˆåˆ†å¸ƒ $p(x_1, x_2, x_3, x_4) = f_a(x_1, x_2) \cdot f_b(x_2, x_3) \cdot f_c(x_3, x_4)$ ç”±é“¾çŠ¶å› å­å›¾è¡¨ç¤ºï¼Œå…¶ä¸­å˜é‡èŠ‚ç‚¹ï¼ˆè“è‰²åœ†åœˆï¼‰å’Œå› å­èŠ‚ç‚¹ï¼ˆçº¢è‰²æ–¹å—ï¼‰ã€‚</p>
<h3>3.1 Why Factor Graphs?</h3>
<p>3.1 ä¸ºä»€ä¹ˆéœ€è¦å› å¼åˆ†è§£å›¾ï¼Ÿ</p>
<p>Factor graphs offer several advantages:
å› å­å›¾å…·æœ‰ä»¥ä¸‹å‡ ä¸ªä¼˜ç‚¹ï¼š</p>
<ol>
<li><strong>Explicit factorization</strong>: Unlike Bayesian networks or MRFs, the factor graph shows exactly which factors connect which variables, even when multiple factors share the same variable set.
<strong>æ˜¾å¼å› å­åˆ†è§£</strong> ï¼šä¸è´å¶æ–¯ç½‘ç»œæˆ– MRF ä¸åŒï¼Œå› å­å›¾å‡†ç¡®åœ°æ˜¾ç¤ºäº†å“ªäº›å› å­è¿æ¥å“ªäº›å˜é‡ï¼Œå³ä½¿å¤šä¸ªå› å­å…±äº«åŒä¸€å˜é‡é›†ã€‚</li>
<li><strong>Unified framework</strong>: Both directed and undirected models can be converted to factor graphs. A Bayesian network's CPTs become factors; an MRF's clique potentials become factors.
<strong>ç»Ÿä¸€æ¡†æ¶</strong> ï¼šæœ‰å‘æ¨¡å‹å’Œæ— å‘æ¨¡å‹å‡å¯è½¬æ¢ä¸ºå› å­å›¾ã€‚è´å¶æ–¯ç½‘ç»œçš„ CPT æˆä¸ºå› å­ï¼›é©¬å°”å¯å¤«éšæœºåœºçš„å›¢åŠ¿æˆä¸ºå› å­ã€‚</li>
<li><strong>Natural setting for message passing</strong>: The bipartite structure of factor graphs directly supports the definition of variable-to-factor and factor-to-variable messages.
<strong>æ¶ˆæ¯ä¼ é€’çš„è‡ªç„¶è®¾ç½®</strong> ï¼šå› å­å›¾çš„äºŒåˆ†ç»“æ„ç›´æ¥æ”¯æŒå®šä¹‰å˜é‡åˆ°å› å­å’Œå› å­åˆ°å˜é‡çš„æ¶ˆæ¯ã€‚</li>
</ol>
<h3>3.2 Notation</h3>
<p>3.2 ç¬¦å·</p>
<p>Throughout this report, we use the following notation:
æœ¬æŠ¥å‘Šä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹ç¬¦å·ï¼š</p>
<table>
<thead>
<tr>
<th>Symbolè±¡å¾</th>
<th>Meaningæ„ä¹‰</th>
</tr>
</thead>
<tbody>
<tr>
<td>xix_ixiâ€‹</td>
<td>Random variable (variable node)éšæœºå˜é‡ï¼ˆå˜é‡èŠ‚ç‚¹ï¼‰</td>
</tr>
<tr>
<td>faf_afaâ€‹</td>
<td>Factor (factor node)å› å­ï¼ˆå› å­èŠ‚ç‚¹ï¼‰</td>
</tr>
<tr>
<td>N(x)N(x)N(x)</td>
<td>Set of factor nodes neighboring variable xxxå˜é‡ xxx ç›¸é‚»çš„å› å­èŠ‚ç‚¹é›†åˆ</td>
</tr>
<tr>
<td>N(f)N(f)N(f)</td>
<td>Set of variable nodes neighboring factor fffç›¸é‚»å› å­ä¸º fff çš„å¯å˜èŠ‚ç‚¹é›†åˆ</td>
</tr>
<tr>
<td>xa\mathbf{x}_axaâ€‹</td>
<td>Set of variables connected to factor faf_afaâ€‹ä¸å› å­ faf_afaâ€‹ ç›¸å…³çš„å˜é‡é›†</td>
</tr>
<tr>
<td>Î¼xâ†’f(x)\mu_{x \to f}(x)Î¼xâ†’fâ€‹(x)</td>
<td>Message from variable xxx to factor fffæ¥è‡ªå˜é‡ xxx çš„æ¶ˆæ¯ç»™å› å­ fff</td>
</tr>
<tr>
<td>Î¼fâ†’x(x)\mu_{f \to x}(x)Î¼fâ†’xâ€‹(x)</td>
<td>Message from factor fff to variable xxxæ¥è‡ªå› å­ fff åˆ°å˜é‡ xxx çš„æ¶ˆæ¯</td>
</tr>
<tr>
<td>b(xi)b(x_i)b(xiâ€‹)</td>
<td>Belief (approximate marginal) at variable xix_ixiâ€‹å˜é‡ xix_ixiâ€‹ å¤„çš„ä¿¡å¿µï¼ˆè¿‘ä¼¼è¾¹é™…ï¼‰</td>
</tr>
</tbody>
</table>
<hr />
<h2>4. Message Passing</h2>
<p>4. æ¶ˆæ¯ä¼ é€’</p>
<p>The core idea of Belief Propagation is <strong>message passing</strong>: nodes in the factor graph exchange local information (messages) with their neighbors, and through iterative exchange, global information about the joint distribution propagates through the network.
ä¿¡å¿µä¼ æ’­çš„æ ¸å¿ƒæ€æƒ³æ˜¯<strong>æ¶ˆæ¯ä¼ é€’</strong> ï¼šå› å­å›¾ä¸­çš„èŠ‚ç‚¹ä¸å…¶é‚»å±…äº¤æ¢å±€éƒ¨ä¿¡æ¯ï¼ˆæ¶ˆæ¯ï¼‰ï¼Œå¹¶é€šè¿‡è¿­ä»£äº¤æ¢ï¼Œå…³äºè”åˆåˆ†å¸ƒçš„å…¨å±€ä¿¡æ¯åœ¨ç½‘ç»œä¸­ä¼ æ’­ã€‚</p>
<div class="video-container">
  <p class="video-caption">Message Passing Mechanism / æ¶ˆæ¯ä¼ é€’æœºåˆ¶</p>
  <video controls preload="metadata">
    <source src="media/videos/bp_scenes/720p30/MessagePassing.mp4" type="video/mp4">
  </video>
  <a class="download-link" href="media/videos/bp_scenes/720p30/MessagePassing.mp4" download>â¬‡ ä¸‹è½½è§†é¢‘</a>
</div>
<h3>4.1 Variable-to-Factor Messages</h3>
<p>4.1 å˜é‡åˆ°å› å­çš„ä¿¡æ¯</p>
<p>A variable node $x$ sends a message to a neighboring factor node $f$ by collecting all incoming messages from its <em>other</em> neighboring factors and multiplying them together:
å˜é‡èŠ‚ç‚¹ $x$ é€šè¿‡æ”¶é›†æ¥è‡ªå…¶<em>å…¶ä»–</em>ç›¸é‚»å› å­çš„æ‰€æœ‰ä¼ å…¥æ¶ˆæ¯å¹¶å°†å®ƒä»¬ç›¸ä¹˜ï¼Œå‘ç›¸é‚»å› å­èŠ‚ç‚¹ $f$ å‘é€æ¶ˆæ¯ï¼š</p>
<p>$\mu_{x \to f}(x) = \prod_{g \in N(x) \setminus f} \mu_{g \to x}(x)$</p>
<p><strong>Intuition</strong>: The variable $x$ tells factor $f$ everything it has learned from all sources <em>except</em> $f$ itself. This prevents information from being &quot;echoed&quot; back to its source.
<strong>ç›´è§‰</strong> ï¼šå˜é‡ $x$ å°†å®ƒä»<em>é™¤è‡ªèº«ä»¥å¤–çš„</em>æ‰€æœ‰æ¥æºå­¦åˆ°çš„æ‰€æœ‰ä¿¡æ¯å‘Šè¯‰å› å­ $f$ ã€‚è¿™å¯ä»¥é˜²æ­¢ä¿¡æ¯â€œå›ä¼ â€åˆ°å…¶æ¥æºã€‚</p>
<p><strong>Special case â€” Leaf variable</strong>: If $x$ is a leaf node (connected to only one factor), then $N(x) \setminus f = \emptyset$, and the message is simply:
<strong>ç‰¹æ®Šæƒ…å†µâ€”â€”å¶å˜é‡</strong> ï¼šå¦‚æœ $x$ æ˜¯ä¸€ä¸ªå¶èŠ‚ç‚¹ï¼ˆä»…è¿æ¥åˆ°ä¸€ä¸ªå› å­ï¼‰ï¼Œåˆ™ $N(x) \setminus f = \emptyset$ ï¼Œæ¶ˆæ¯å†…å®¹ä¸ºï¼š</p>
<p>$\mu_{x \to f}(x) = 1 \quad \text{(uniform message)}$</p>
<h3>4.2 Factor-to-Variable Messages</h3>
<p>4.2 å› å­åˆ°å˜é‡çš„ä¿¡æ¯</p>
<p>A factor node $f$ sends a message to a neighboring variable node $x$ by:
å› å­èŠ‚ç‚¹ $f$ é€šè¿‡ä»¥ä¸‹æ–¹å¼å‘ç›¸é‚»å˜é‡èŠ‚ç‚¹ $x$ å‘é€æ¶ˆæ¯ï¼š</p>
<ol>
<li>Multiplying the factor $f(\mathbf{x}_f)$ with all incoming messages from neighboring variables <em>except</em> $x$
å°†å› å­ $f(\mathbf{x}_f)$ ä¸<em>é™¤</em> $x$ ä»¥å¤–çš„æ‰€æœ‰ç›¸é‚»å˜é‡çš„ä¼ å…¥æ¶ˆæ¯ç›¸ä¹˜</li>
<li>Summing (marginalizing) over all variables <em>except</em> $x$
å¯¹é™¤ $x$ <em>ä¹‹å¤–çš„</em>æ‰€æœ‰å˜é‡æ±‚å’Œï¼ˆè¾¹ç¼˜åŒ–ï¼‰</li>
</ol>
<p>$\mu_{f \to x}(x) = \sum_{\sim x} f(\mathbf{x}<em>f) \prod</em>{y \in N(f) \setminus x} \mu_{y \to f}(y)$</p>
<p>where $\sum_{\sim x}$ denotes summation over all variables in $\mathbf{x}_f$ except $x$.
å…¶ä¸­ âˆ‘ âˆ¼ ğ‘¥ âˆ‘ âˆ¼x â€‹ è¡¨ç¤ºå¯¹ ğ‘¥ ä¸­æ‰€æœ‰å˜é‡æ±‚å’Œ ğ‘“ x f â€‹ é™¤äº† $x$ ã€‚</p>
<p><strong>Intuition</strong>: The factor $f$ summarizes how all its other connected variables interact through it, and communicates this summary to $x$.
<strong>ç›´è§‰</strong> ï¼šå› å­ $f$ æ¦‚æ‹¬äº†æ‰€æœ‰å…¶ä»–ä¸å…¶ç›¸å…³çš„å˜é‡å¦‚ä½•é€šè¿‡å®ƒç›¸äº’ä½œç”¨ï¼Œå¹¶å°†æ­¤æ¦‚æ‹¬ä¼ è¾¾ç»™ $x$ ã€‚</p>
<h3>4.3 Belief Computation</h3>
<p>4.3 ä¿¡å¿µè®¡ç®—</p>
<p>After all messages have been exchanged, the <strong>belief</strong> (approximate marginal) at each variable node is computed as the product of all incoming factor-to-variable messages:
æ‰€æœ‰æ¶ˆæ¯äº¤æ¢å®Œæ¯•åï¼Œæ¯ä¸ªå˜é‡èŠ‚ç‚¹çš„<strong>ç½®ä¿¡åº¦</strong> ï¼ˆè¿‘ä¼¼è¾¹ç¼˜ç½®ä¿¡åº¦ï¼‰è®¡ç®—ä¸ºæ‰€æœ‰ä¼ å…¥çš„å› å­åˆ°å˜é‡æ¶ˆæ¯çš„ä¹˜ç§¯ï¼š</p>
<p>$b(x_i) \propto \prod_{f \in N(x_i)} \mu_{f \to x_i}(x_i)$</p>
<p>The belief $b(x_i)$ is then normalized to be a valid probability distribution.
ç„¶åï¼Œå°†ä¿¡å¿µ $b(x_i)$ å½’ä¸€åŒ–ä¸ºä¸€ä¸ªæœ‰æ•ˆçš„æ¦‚ç‡åˆ†å¸ƒã€‚</p>
<hr />
<h2>5. The Sum-Product Algorithm</h2>
<p>5. å’Œç§¯ç®—æ³•</p>
<p>The <strong>Sum-Product algorithm</strong> is the concrete instantiation of Belief Propagation for computing <strong>marginal probabilities</strong>. It is called &quot;Sum-Product&quot; because the factor-to-variable message involves a <em>sum</em> (marginalization) of a <em>product</em> (factor times incoming messages).
<strong>æ±‚å’Œ-ä¹˜ç§¯ç®—æ³•</strong>æ˜¯ç½®ä¿¡ä¼ æ’­ç®—æ³•è®¡ç®—<strong>è¾¹ç¼˜æ¦‚ç‡</strong>çš„å…·ä½“å®ç°ã€‚ä¹‹æ‰€ä»¥ç§°ä¸ºâ€œæ±‚å’Œ-ä¹˜ç§¯â€ï¼Œæ˜¯å› ä¸ºå› å­åˆ°å˜é‡çš„æ¶ˆæ¯ä¼ é€’æ¶‰åŠ<em>ä¹˜ç§¯</em> ï¼ˆå› å­ä¹˜ä»¥ä¼ å…¥æ¶ˆæ¯ï¼‰çš„<em>æ±‚å’Œ</em> ï¼ˆè¾¹ç¼˜åŒ–ï¼‰ã€‚</p>
<div class="video-container">
  <p class="video-caption">Sum-Product Algorithm / å’Œç§¯ç®—æ³•</p>
  <video controls preload="metadata">
    <source src="media/videos/bp_scenes/720p30/SumProductAlgorithm.mp4" type="video/mp4">
  </video>
  <a class="download-link" href="media/videos/bp_scenes/720p30/SumProductAlgorithm.mp4" download>â¬‡ ä¸‹è½½è§†é¢‘</a>
</div>
<h3>5.1 Algorithm on Trees</h3>
<p>5.1 æ ‘ä¸Šçš„ç®—æ³•</p>
<p>On tree-structured factor graphs (no cycles), the Sum-Product algorithm proceeds in two passes:
åœ¨æ ‘çŠ¶ç»“æ„çš„å› å­å›¾ï¼ˆæ— ç¯ï¼‰ä¸Šï¼Œæ±‚å’Œ-ä¹˜ç§¯ç®—æ³•åˆ†ä¸¤æ­¥è¿›è¡Œï¼š</p>
<h4>Forward Pass (Leaves to Root)</h4>
<p>å‰å‘ä¼ é€’ï¼ˆä»å¶å­åˆ°æ ¹ï¼‰</p>
<ol>
<li>Choose an arbitrary root node
é€‰æ‹©ä¸€ä¸ªä»»æ„æ ¹èŠ‚ç‚¹</li>
<li>Starting from the leaf nodes, send messages toward the root
ä»å¶èŠ‚ç‚¹å¼€å§‹ï¼Œå‘æ ¹èŠ‚ç‚¹å‘é€æ¶ˆæ¯ã€‚</li>
<li>Each node sends its message only after it has received all incoming messages from its children
æ¯ä¸ªèŠ‚ç‚¹åªæœ‰åœ¨æ”¶åˆ°æ‰€æœ‰æ¥è‡ªå…¶å­èŠ‚ç‚¹çš„æ¶ˆæ¯åæ‰ä¼šå‘é€è‡ªå·±çš„æ¶ˆæ¯ã€‚</li>
</ol>
<h4>Backward Pass (Root to Leaves)</h4>
<p>åå‘ä¼ é€’ï¼ˆä»æ ¹åˆ°å¶ï¼‰</p>
<ol start="4">
<li>The root sends messages back to its children
æ ¹èŠ‚ç‚¹ä¼šå‘å…¶å­èŠ‚ç‚¹å‘é€æ¶ˆæ¯</li>
<li>Messages propagate outward until they reach all leaf nodes
ä¿¡æ¯å‘å¤–ä¼ æ’­ï¼Œç›´åˆ°åˆ°è¾¾æ‰€æœ‰å¶èŠ‚ç‚¹ã€‚</li>
</ol>
<h4>Marginal Computation</h4>
<p>è¾¹é™…è®¡ç®—</p>
<ol start="6">
<li>At each variable node $x_i$, the marginal is computed as:
åœ¨æ¯ä¸ªå˜é‡èŠ‚ç‚¹ğ‘¥ ğ‘– x i â€‹ è¾¹é™…æ•ˆåº”çš„è®¡ç®—æ–¹æ³•å¦‚ä¸‹ï¼š</li>
</ol>
<p>$p(x_i) = \frac{1}{Z_i} \prod_{f \in N(x_i)} \mu_{f \to x_i}(x_i)$</p>
<p>where $Z_i$ is a normalization constant.
å…¶ä¸­ğ‘ ğ‘– Z i â€‹ æ˜¯å½’ä¸€åŒ–å¸¸æ•°ã€‚</p>
<h3>5.2 Correctness on Trees</h3>
<p>5.2 æ ‘ä¸Šçš„æ­£ç¡®æ€§</p>
<p><strong>Theorem</strong>: On a tree-structured factor graph, the Sum-Product algorithm computes the <strong>exact</strong> marginal distributions for all variables after a single forward-backward pass.
<strong>å®šç†</strong> ï¼šåœ¨æ ‘çŠ¶ç»“æ„çš„å› å­å›¾ä¸Šï¼Œæ±‚å’Œ-ä¹˜ç§¯ç®—æ³•åœ¨ä¸€æ¬¡å‰å‘-åå‘ä¼ é€’åå³å¯è®¡ç®—å‡ºæ‰€æœ‰å˜é‡çš„<strong>ç²¾ç¡®</strong>è¾¹ç¼˜åˆ†å¸ƒã€‚</p>
<p><strong>Proof sketch</strong>: On a tree, every path between two nodes is unique. Therefore, when a node computes its belief using incoming messages, each piece of information (from each factor) is counted exactly once. There is no &quot;double-counting&quot; â€” the fundamental problem that arises in graphs with cycles.
<strong>è¯æ˜æ¦‚è¦</strong> ï¼šåœ¨æ ‘çŠ¶å›¾ä¸­ï¼Œä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´çš„æ¯æ¡è·¯å¾„éƒ½æ˜¯å”¯ä¸€çš„ã€‚å› æ­¤ï¼Œå½“ä¸€ä¸ªèŠ‚ç‚¹ä½¿ç”¨ä¼ å…¥çš„æ¶ˆæ¯è®¡ç®—å…¶ä¿¡å¿µæ—¶ï¼Œæ¯ä¸ªä¿¡æ¯ï¼ˆæ¥è‡ªæ¯ä¸ªå› å­ï¼‰éƒ½æ°å¥½è¢«è®¡æ•°ä¸€æ¬¡ã€‚ä¸å­˜åœ¨â€œé‡å¤è®¡æ•°â€â€”â€”è¿™æ˜¯å¸¦ç¯å›¾ä¸­å‡ºç°çš„æ ¹æœ¬é—®é¢˜ã€‚</p>
<h3>5.3 Complexity</h3>
<p>5.3 å¤æ‚æ€§</p>
<p>For a tree with $n$ variable nodes, where each variable takes at most $k$ values and each factor connects at most $d$ variables:
å¯¹äºä¸€ä¸ªå…·æœ‰ $n$ ä¸ªå˜é‡èŠ‚ç‚¹çš„æ ‘ï¼Œå…¶ä¸­æ¯ä¸ªå˜é‡æœ€å¤šå– $k$ ä¸ªå€¼ï¼Œå¹¶ä¸”æ¯ä¸ªå› å­æœ€å¤šè¿æ¥ $d$ ä¸ªå˜é‡ï¼š</p>
<ul>
<li><strong>Message computation</strong>: $O(k^d)$ per message (summing over neighbor configurations)
<strong>æ¶ˆæ¯è®¡ç®—</strong> ï¼šæ¯æ¡æ¶ˆæ¯ $O(k^d)$ ï¼ˆå¯¹é‚»å±…é…ç½®æ±‚å’Œï¼‰</li>
<li><strong>Total messages</strong>: $O(n)$ (two messages per edge, one in each direction)
<strong>æ¶ˆæ¯æ€»æ•°</strong> ï¼š $O(n)$ ï¼ˆæ¯æ¡è¾¹ä¸¤æ¡æ¶ˆæ¯ï¼Œæ¯ä¸ªæ–¹å‘ä¸€æ¡ï¼‰</li>
<li><strong>Overall complexity</strong>: $O(n \cdot k^d)$, which is linear in the number of variables â€” a dramatic improvement over the brute-force $O(k^n)$.
<strong>æ€»ä½“å¤æ‚åº¦</strong> ï¼š $O(n \cdot k^d)$ ï¼Œä¸å˜é‡æ•°é‡å‘ˆçº¿æ€§å…³ç³»â€”â€”æ¯”æš´åŠ›æœç´¢ $O(k^n)$ æœ‰äº†æ˜¾è‘—æ”¹è¿›ã€‚</li>
</ul>
<hr />
<h2>6. The Max-Product Algorithm</h2>
<p>6. æœ€å¤§ä¹˜ç§¯ç®—æ³•</p>
<p>While the Sum-Product algorithm computes marginal probabilities, many applications require finding the <strong>Maximum A Posteriori (MAP)</strong> assignment â€” the single most probable configuration of all variables:
è™½ç„¶æ±‚å’Œä¹˜ç§¯ç®—æ³•è®¡ç®—çš„æ˜¯è¾¹ç¼˜æ¦‚ç‡ï¼Œä½†è®¸å¤šåº”ç”¨éœ€è¦æ‰¾åˆ°<strong>æœ€å¤§åéªŒæ¦‚ç‡ (MAP)</strong> åˆ†é…â€”â€”æ‰€æœ‰å˜é‡çš„æœ€å¯èƒ½é…ç½®ï¼š</p>
<p>$\mathbf{x}^* = \arg\max_{\mathbf{x}} p(\mathbf{x}) = \arg\max_{\mathbf{x}} \prod_a f_a(\mathbf{x}_a)$</p>
<p>The <strong>Max-Product algorithm</strong> achieves this by replacing the summation in the factor-to-variable message with a maximization:
<strong>æœ€å¤§ä¹˜ç§¯ç®—æ³•</strong>é€šè¿‡å°†å› å­åˆ°å˜é‡æ¶ˆæ¯ä¸­çš„æ±‚å’Œæ›¿æ¢ä¸ºæœ€å¤§åŒ–æ¥å®ç°è¿™ä¸€ç‚¹ï¼š</p>
<p><div class="image-container"><img src="media/images/bp_scenes/MaxProductComparison_ManimCE_v0.19.2.png" alt="Sum-Product vs Max-Product Comparison"><a class="download-link" href="media/images/bp_scenes/MaxProductComparison_ManimCE_v0.19.2.png" download>â¬‡ ä¸‹è½½å›¾ç‰‡</a></div></p>
<h3>6.1 Message Update Rules</h3>
<p>6.1 æ¶ˆæ¯æ›´æ–°è§„åˆ™</p>
<p><strong>Variable-to-Factor</strong> (same as Sum-Product):
<strong>å˜é‡åˆ°å› å­</strong> ï¼ˆä¸å’Œ-ç§¯ç›¸åŒï¼‰ï¼š</p>
<p>$\mu_{x \to f}(x) = \prod_{g \in N(x) \setminus f} \mu_{g \to x}(x)$</p>
<p><strong>Factor-to-Variable</strong> (max replaces sum):
<strong>å› å­åˆ°å˜é‡çš„è½¬æ¢</strong> ï¼ˆæœ€å¤§å€¼ä»£æ›¿æ€»å’Œï¼‰ï¼š</p>
<p>$\mu_{f \to x}(x) = \max_{\sim x} \left[ f(\mathbf{x}<em>f) \prod</em>{y \in N(f) \setminus x} \mu_{y \to f}(y) \right]$</p>
<h3>6.2 MAP Estimation</h3>
<p>6.2 MAP ä¼°è®¡</p>
<p>After convergence, the MAP estimate at each variable is:
æ”¶æ•›åï¼Œæ¯ä¸ªå˜é‡çš„æœ€å¤§åéªŒæ¦‚ç‡ä¼°è®¡å€¼ä¸ºï¼š</p>
<p>$x_i^* = \arg\max_{x_i} \prod_{f \in N(x_i)} \mu_{f \to x_i}(x_i)$</p>
<h3>6.3 Min-Sum (Log-Domain) Variant</h3>
<p>6.3 æœ€å°å’Œï¼ˆå¯¹æ•°åŸŸï¼‰å˜ä½“</p>
<p>In practice, it is often more numerically stable to work in the <strong>log domain</strong>. Taking the negative logarithm transforms the Max-Product algorithm into the <strong>Min-Sum</strong> algorithm:
å®é™…ä¸Šï¼Œåœ¨å¯¹<strong>æ•°åŸŸä¸­</strong>è¿›è¡Œè¿ç®—é€šå¸¸æ•°å€¼ç¨³å®šæ€§æ›´é«˜ã€‚å–è´Ÿå¯¹æ•°å¯ä»¥å°†æœ€å¤§ä¹˜ç§¯ç®—æ³•è½¬åŒ–ä¸º<strong>æœ€å°å’Œ</strong>ç®—æ³•ï¼š</p>
<ul>
<li>Products become sums: $\log(a \cdot b) = \log a + \log b$
ä¹˜ç§¯å˜ä¸ºå’Œï¼š $\log(a \cdot b) = \log a + \log b$</li>
<li>Maximization becomes minimization (with negation): $\max \to \min$
æœ€å¤§åŒ–å˜ä¸ºæœ€å°åŒ–ï¼ˆå¸¦å¦å®šï¼‰ï¼š $\max \to \min$</li>
</ul>
<p>This avoids numerical underflow issues that arise when multiplying many small probabilities, and is closely related to the <strong>Viterbi algorithm</strong> for finding the most likely sequence in Hidden Markov Models.
è¿™æ ·å°±é¿å…äº†å°†è®¸å¤šå°æ¦‚ç‡ç›¸ä¹˜æ—¶å‡ºç°çš„æ•°å€¼ä¸‹æº¢é—®é¢˜ï¼Œå¹¶ä¸”ä¸ç”¨äºåœ¨éšé©¬å°”å¯å¤«æ¨¡å‹ä¸­å¯»æ‰¾æœ€å¯èƒ½åºåˆ—çš„<strong>ç»´ç‰¹æ¯”ç®—æ³•</strong>å¯†åˆ‡ç›¸å…³ã€‚</p>
<h3>6.4 Correctness</h3>
<p>6.4 æ­£ç¡®æ€§</p>
<p>On tree-structured graphs, the Max-Product algorithm finds the <strong>exact</strong> MAP assignment. On loopy graphs, it provides an approximation.
å¯¹äºæ ‘çŠ¶å›¾ï¼Œæœ€å¤§ä¹˜ç§¯ç®—æ³•å¯ä»¥æ‰¾åˆ°<strong>ç²¾ç¡®çš„</strong>æœ€å¤§åéªŒæ¦‚ç‡ (MAP) åˆ†é…ã€‚å¯¹äºç¯çŠ¶å›¾ï¼Œå®ƒåªèƒ½æä¾›è¿‘ä¼¼å€¼ã€‚</p>
<hr />
<h2>7. Exact Inference on Trees</h2>
<p>7. æ ‘ä¸Šçš„ç²¾ç¡®æ¨ç†</p>
<p>Tree-structured factor graphs are special because Belief Propagation yields <strong>exact</strong> results. This section formalizes why this is the case and describes the two-pass message schedule.
æ ‘çŠ¶ç»“æ„çš„å› å­å›¾ä¹‹æ‰€ä»¥ç‰¹æ®Šï¼Œæ˜¯å› ä¸ºç½®ä¿¡ä¼ æ’­èƒ½å¤Ÿäº§ç”Ÿ<strong>ç²¾ç¡®</strong>ç»“æœã€‚æœ¬èŠ‚å°†é˜è¿°å…¶åŸå› ï¼Œå¹¶æè¿°ä¸¤éæ¶ˆæ¯è°ƒåº¦ã€‚</p>
<div class="video-container">
  <p class="video-caption">Exact Inference on Trees / æ ‘ä¸Šçš„ç²¾ç¡®æ¨ç†</p>
  <video controls preload="metadata">
    <source src="media/videos/bp_scenes/720p30/TreeBP.mp4" type="video/mp4">
  </video>
  <a class="download-link" href="media/videos/bp_scenes/720p30/TreeBP.mp4" download>â¬‡ ä¸‹è½½è§†é¢‘</a>
</div>
<h3>7.1 Two-Pass Message Schedule</h3>
<p>7.1 ä¸¤éæ¶ˆæ¯è°ƒåº¦</p>
<p>Given a tree-structured factor graph:
ç»™å®šä¸€ä¸ªæ ‘çŠ¶ç»“æ„çš„å› å­å›¾ï¼š</p>
<p><strong>Pass 1 (Leaves â†’ Root):
ç¬¬ä¸€é˜¶æ®µï¼ˆå¶â†’æ ¹ï¼‰ï¼š</strong></p>
<ol>
<li>Select any variable node as the root
é€‰æ‹©ä»»æ„å˜é‡èŠ‚ç‚¹ä½œä¸ºæ ¹èŠ‚ç‚¹</li>
<li>All leaf nodes send their messages (uniform for variables, or the factor value for factor leaves)
æ‰€æœ‰å¶å­èŠ‚ç‚¹å‘é€æ¶ˆæ¯ï¼ˆå˜é‡å¶å­èŠ‚ç‚¹å‘é€ç»Ÿä¸€æ¶ˆæ¯ï¼Œå› å­å¶å­èŠ‚ç‚¹å‘é€å› å­å€¼æ¶ˆæ¯ï¼‰ã€‚</li>
<li>Each non-leaf node waits to receive messages from all children, then sends a single message to its parent
æ¯ä¸ªéå¶å­èŠ‚ç‚¹ç­‰å¾…æ¥æ”¶æ¥è‡ªæ‰€æœ‰å­èŠ‚ç‚¹çš„æ¶ˆæ¯ï¼Œç„¶åå‘å…¶çˆ¶èŠ‚ç‚¹å‘é€ä¸€æ¡æ¶ˆæ¯ã€‚</li>
<li>This continues until the root has received messages from all children
è¿™ä¸ªè¿‡ç¨‹ä¼šä¸€ç›´æŒç»­åˆ°æ ¹èŠ‚ç‚¹æ”¶åˆ°æ‰€æœ‰å­èŠ‚ç‚¹çš„æ¶ˆæ¯ä¸ºæ­¢ã€‚</li>
</ol>
<p><strong>Pass 2 (Root â†’ Leaves):</strong> 5. The root sends messages to all its children 6. Each node, upon receiving a message from its parent, sends messages to all its children 7. This continues until all leaf nodes have received messages
<strong>ç¬¬äºŒé˜¶æ®µï¼ˆæ ¹èŠ‚ç‚¹â†’å¶èŠ‚ç‚¹ï¼‰ï¼š</strong> 5. æ ¹èŠ‚ç‚¹å‘å…¶æ‰€æœ‰å­èŠ‚ç‚¹å‘é€æ¶ˆæ¯ã€‚6. æ¯ä¸ªèŠ‚ç‚¹åœ¨æ”¶åˆ°å…¶çˆ¶èŠ‚ç‚¹çš„æ¶ˆæ¯åï¼Œå‘å…¶æ‰€æœ‰å­èŠ‚ç‚¹å‘é€æ¶ˆæ¯ã€‚7. æ­¤è¿‡ç¨‹æŒç»­è¿›è¡Œï¼Œç›´åˆ°æ‰€æœ‰å¶èŠ‚ç‚¹éƒ½æ”¶åˆ°æ¶ˆæ¯ä¸ºæ­¢ã€‚</p>
<p>After both passes, every edge has carried exactly two messages (one in each direction), and every node can compute its exact marginal.
ç»è¿‡ä¸¤æ¬¡ä¼ é€’åï¼Œæ¯æ¡è¾¹éƒ½æ°å¥½æ‰¿è½½äº†ä¸¤æ¡æ¶ˆæ¯ï¼ˆæ¯ä¸ªæ–¹å‘ä¸€æ¡ï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥è®¡ç®—å…¶ç²¾ç¡®çš„è¾¹ç¼˜åˆ†å¸ƒã€‚</p>
<h3>7.2 Why Trees Are Special</h3>
<p>7.2 ä¸ºä»€ä¹ˆæ ‘æœ¨å¦‚æ­¤ç‰¹åˆ«</p>
<p>The key property of trees is that they contain <strong>no cycles</strong>. This means:
æ ‘çš„å…³é”®ç‰¹æ€§æ˜¯å®ƒä»¬<strong>ä¸åŒ…å«ç¯è·¯</strong> ã€‚è¿™æ„å‘³ç€ï¼š</p>
<ol>
<li><strong>Unique paths</strong>: There is exactly one path between any two nodes
<strong>å”¯ä¸€è·¯å¾„</strong> ï¼šä»»æ„ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´éƒ½åªæœ‰ä¸€æ¡è·¯å¾„ã€‚</li>
<li><strong>No double-counting</strong>: Each factor contributes to a variable's belief exactly once
<strong>ä¸é‡å¤è®¡ç®—</strong> ï¼šæ¯ä¸ªå› ç´ å¯¹å˜é‡çš„ç½®ä¿¡åº¦ä»…è´¡çŒ®ä¸€æ¬¡ã€‚</li>
<li><strong>Convergence in finite steps</strong>: The two-pass schedule terminates after visiting each edge twice
<strong>æœ‰é™æ­¥æ”¶æ•›</strong> ï¼šä¸¤éè°ƒåº¦ç®—æ³•åœ¨è®¿é—®æ¯æ¡è¾¹ä¸¤æ¬¡åç»ˆæ­¢ã€‚</li>
</ol>
<p>In contrast, graphs with cycles can cause messages to reinforce themselves, leading to the double-counting of evidence â€” the fundamental challenge addressed by Loopy BP.
ç›¸æ¯”ä¹‹ä¸‹ï¼Œå¸¦æœ‰å¾ªç¯çš„å›¾ä¼šå¯¼è‡´ä¿¡æ¯è‡ªæˆ‘å¼ºåŒ–ï¼Œä»è€Œå¯¼è‡´è¯æ®é‡å¤è®¡ç®—â€”â€”è¿™æ˜¯ Loopy BP æ‰€è§£å†³çš„æ ¹æœ¬æŒ‘æˆ˜ã€‚</p>
<h3>7.3 Junction Tree Algorithm</h3>
<p>7.3 è¿æ¥æ ‘ç®—æ³•</p>
<p>For general graphs, exact inference can still be performed by converting the graph into a <strong>junction tree</strong> (also called a clique tree) through a process of triangulation and clique identification. The Sum-Product algorithm on the junction tree yields exact marginals. However, the complexity depends on the <strong>treewidth</strong> of the graph â€” for graphs with large treewidth, this approach becomes intractable, motivating approximate methods like Loopy BP.
å¯¹äºä¸€èˆ¬å›¾ï¼Œå¯ä»¥é€šè¿‡ä¸‰è§’å‰–åˆ†å’Œå›¢è¯†åˆ«è¿‡ç¨‹å°†å›¾è½¬æ¢ä¸º<strong>è¿æ¥æ ‘</strong> ï¼ˆä¹Ÿç§°ä¸ºå›¢æ ‘ï¼‰ï¼Œä»è€Œå®ç°ç²¾ç¡®æ¨ç†ã€‚è¿æ¥æ ‘ä¸Šçš„å’Œç§¯ç®—æ³•å¯ä»¥å¾—åˆ°ç²¾ç¡®çš„è¾¹ç¼˜åˆ†å¸ƒã€‚ç„¶è€Œï¼Œå…¶å¤æ‚åº¦å–å†³äºå›¾çš„<strong>æ ‘å®½</strong> â€”â€”å¯¹äºæ ‘å®½è¾ƒå¤§çš„å›¾ï¼Œè¿™ç§æ–¹æ³•å˜å¾—éš¾ä»¥å¤„ç†ï¼Œå› æ­¤éœ€è¦ä½¿ç”¨è¯¸å¦‚ Loopy BP ä¹‹ç±»çš„è¿‘ä¼¼æ–¹æ³•ã€‚</p>
<hr />
<h2>8. Loopy Belief Propagation</h2>
<p>8. å¾ªç¯ä¿¡å¿µä¼ æ’­</p>
<p>When the factor graph contains <strong>cycles</strong> (loops), the standard two-pass schedule cannot be applied, and messages may travel around loops indefinitely. <strong>Loopy Belief Propagation</strong> (LBP) applies the same message update rules iteratively until (approximate) convergence.
å½“å› å­å›¾åŒ…å«<strong>ç¯è·¯</strong>æ—¶ï¼Œæ ‡å‡†çš„ä¸¤éè°ƒåº¦æ–¹æ³•æ— æ³•åº”ç”¨ï¼Œæ¶ˆæ¯å¯èƒ½ä¼šæ— é™å¾ªç¯åœ°åœ¨ç¯è·¯ä¸­ä¼ æ’­ã€‚ <strong>å¾ªç¯ç½®ä¿¡ä¼ æ’­</strong> ï¼ˆLBPï¼‰ä¼šè¿­ä»£åœ°åº”ç”¨ç›¸åŒçš„æ¶ˆæ¯æ›´æ–°è§„åˆ™ï¼Œç›´åˆ°ï¼ˆè¿‘ä¼¼ï¼‰æ”¶æ•›ã€‚</p>
<div class="video-container">
  <p class="video-caption">Loopy BP on a Cyclic Graph / å¾ªç¯å›¾ä¸Šçš„å¾ªç¯ BP</p>
  <video controls preload="metadata">
    <source src="media/videos/bp_scenes/720p30/LoopyBP.mp4" type="video/mp4">
  </video>
  <a class="download-link" href="media/videos/bp_scenes/720p30/LoopyBP.mp4" download>â¬‡ ä¸‹è½½è§†é¢‘</a>
</div>
<h3>8.1 Algorithm</h3>
<p>8.1 ç®—æ³•</p>
<ol>
<li><strong>Initialize</strong> all messages to uniform distributions (or random)
å°†æ‰€æœ‰æ¶ˆæ¯<strong>åˆå§‹åŒ–</strong>ä¸ºå‡åŒ€åˆ†å¸ƒï¼ˆæˆ–éšæœºåˆ†å¸ƒï¼‰ã€‚</li>
<li><strong>Iterate</strong>: For each edge in the graph, update the messages using the Sum-Product (or Max-Product) update rules
<strong>è¿­ä»£</strong> ï¼šå¯¹äºå›¾ä¸­çš„æ¯æ¡è¾¹ï¼Œä½¿ç”¨æ±‚å’Œ-ä¹˜ç§¯ï¼ˆæˆ–æœ€å¤§ä¹˜ç§¯ï¼‰æ›´æ–°è§„åˆ™æ›´æ–°æ¶ˆæ¯ã€‚</li>
<li><strong>Repeat</strong> until messages change by less than a threshold $\epsilon$, or a maximum number of iterations is reached
<strong>é‡å¤æ­¤è¿‡ç¨‹</strong> ï¼Œç›´åˆ°æ¶ˆæ¯å˜åŒ–å°äºé˜ˆå€¼ $\epsilon$ ï¼Œæˆ–è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ä¸ºæ­¢ã€‚</li>
<li><strong>Compute beliefs</strong> from the final messages
æ ¹æ®æœ€ç»ˆæ¶ˆæ¯<strong>è®¡ç®—ç½®ä¿¡åº¦</strong></li>
</ol>
<h3>8.2 Message Schedule</h3>
<p>8.2 æ¶ˆæ¯è®¡åˆ’</p>
<p>Several scheduling strategies exist:
å­˜åœ¨å¤šç§æ’è¯¾ç­–ç•¥ï¼š</p>
<table>
<thead>
<tr>
<th>Scheduleæ—¥ç¨‹</th>
<th>Descriptionæè¿°</th>
</tr>
</thead>
<tbody>
<tr>
<td>Synchronous (Flooding)åŒæ­¥ï¼ˆæ³›æ´ªï¼‰</td>
<td>All messages updated simultaneously in each iterationæ¯æ¬¡è¿­ä»£ä¸­æ‰€æœ‰æ¶ˆæ¯åŒæ—¶æ›´æ–°</td>
</tr>
<tr>
<td>Asynchronous (Sequential)å¼‚æ­¥ï¼ˆé¡ºåºï¼‰</td>
<td>Messages updated one at a time in some orderæ¶ˆæ¯æŒ‰æŸç§é¡ºåºé€æ¡æ›´æ–°ã€‚</td>
</tr>
<tr>
<td>Residual BPæ®‹ä½™è¡€å‹</td>
<td>Prioritize updating messages with largest residual (change)ä¼˜å…ˆæ›´æ–°å˜åŒ–é‡æœ€å¤§çš„æ¶ˆæ¯</td>
</tr>
</tbody>
</table>
<h3>8.3 Convergence Properties</h3>
<p>8.3 æ”¶æ•›æ€§è´¨</p>
<p>Unlike the tree case, Loopy BP has <strong>no general convergence guarantee</strong>:
ä¸æ ‘å½¢é—®é¢˜ä¸åŒï¼ŒLoopy BP <strong>æ²¡æœ‰ä¸€èˆ¬çš„æ”¶æ•›æ€§ä¿è¯</strong> ï¼š</p>
<ul>
<li>On some graphs, messages converge to a fixed point that provides excellent marginal approximations
åœ¨æŸäº›å›¾ä¸Šï¼Œæ¶ˆæ¯ä¼šæ”¶æ•›åˆ°ä¸€ä¸ªå›ºå®šç‚¹ï¼Œè¯¥å›ºå®šç‚¹æä¾›äº†æä½³çš„è¾¹ç¼˜è¿‘ä¼¼å€¼ã€‚</li>
<li>On others, messages may <strong>oscillate</strong> or even <strong>diverge</strong>
åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œä¿¡æ¯å¯èƒ½ä¼š<strong>æ³¢åŠ¨</strong>ç”šè‡³<strong>å‡ºç°åˆ†æ­§ã€‚</strong></li>
<li>Convergence is more likely when:
å½“å‡ºç°ä»¥ä¸‹æƒ…å†µæ—¶ï¼Œè¶‹åŒçš„å¯èƒ½æ€§æ›´å¤§ï¼š
<ul>
<li>The graph has long loops (weak interactions around cycles)
è¯¥å›¾å­˜åœ¨é•¿ç¯ï¼ˆç¯å‘¨å›´çš„ç›¸äº’ä½œç”¨è¾ƒå¼±ï¼‰ã€‚</li>
<li>The factors/potentials are &quot;weak&quot; (close to uniform)
è¿™äº›å› ç´ /æ½œåŠ›â€œè¾ƒå¼±â€ï¼ˆæ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼‰ã€‚</li>
<li><strong>Damping</strong> is applied: $\mu^{(t+1)} = \alpha \cdot \mu^{\text{new}} + (1-\alpha) \cdot \mu^{(t)}$
åº”ç”¨<strong>é˜»å°¼</strong> ï¼š $\mu^{(t+1)} = \alpha \cdot \mu^{\text{new}} + (1-\alpha) \cdot \mu^{(t)}$</li>
</ul>
</li>
</ul>
<h3>8.4 Theoretical Foundations</h3>
<p>8.4 ç†è®ºåŸºç¡€</p>
<p>When Loopy BP converges, the fixed point can be characterized as a stationary point of the <strong>Bethe free energy</strong>:
å½“ Loopy BP æ”¶æ•›æ—¶ï¼Œä¸åŠ¨ç‚¹å¯ä»¥è¢«æè¿°ä¸º <strong>Bethe è‡ªç”±èƒ½</strong>çš„é©»ç‚¹ï¼š</p>
<p>$F_{\text{Bethe}} = \sum_a \sum_{\mathbf{x}_a} b_a(\mathbf{x}_a) \left[ \ln b_a(\mathbf{x}_a) - \ln f_a(\mathbf{x}<em>a) \right] - \sum_i (d_i - 1) \sum</em>{x_i} b_i(x_i) \ln b_i(x_i)$</p>
<p>where $b_a$ and $b_i$ are the factor and variable beliefs, and $d_i$ is the degree of variable node $i$. This connection to variational inference (Yedidia, Freeman, and Weiss, 2001) provides theoretical justification for Loopy BP and has led to improved variants.
å…¶ä¸­ ğ‘ ğ‘ b a â€‹ å’Œ ğ‘ ğ‘– b i â€‹ æ˜¯å› ç´ å’Œå˜é‡ä¿¡å¿µï¼Œä»¥åŠ ğ‘‘ ğ‘– d i â€‹ æ˜¯å˜é‡èŠ‚ç‚¹ $i$ çš„åº¦ã€‚è¿™ç§ä¸å˜åˆ†æ¨æ–­ï¼ˆYedidiaã€Freeman å’Œ Weissï¼Œ2001ï¼‰çš„è”ç³»ä¸º Loopy BP æä¾›äº†ç†è®ºä¾æ®ï¼Œå¹¶å¯¼è‡´äº†æ”¹è¿›çš„å˜ä½“ã€‚</p>
<h3>8.5 Practical Considerations</h3>
<p>8.5 å®é™…è€ƒè™‘å› ç´ </p>
<p>Despite lacking formal guarantees, Loopy BP is remarkably effective in practice:
å°½ç®¡ç¼ºä¹æ­£å¼çš„ä¿è¯ï¼ŒLoopy BP åœ¨å®è·µä¸­å´éå¸¸æœ‰æ•ˆï¼š</p>
<ul>
<li><strong>Turbo codes</strong> and <strong>LDPC codes</strong>: Near-Shannon-limit performance in decoding
<strong>Turbo ç </strong>å’Œ <strong>LDPC ç </strong> ï¼šè¯‘ç æ€§èƒ½æ¥è¿‘é¦™å†œæé™</li>
<li><strong>Stereo vision</strong>: State-of-the-art depth estimation
<strong>ç«‹ä½“è§†è§‰</strong> ï¼šæœ€å…ˆè¿›çš„æ·±åº¦ä¼°è®¡</li>
<li><strong>Protein folding</strong>: Prediction of molecular structures
<strong>è›‹ç™½è´¨æŠ˜å </strong> ï¼šåˆ†å­ç»“æ„é¢„æµ‹</li>
</ul>
<p>The empirical success of Loopy BP, combined with its simplicity and efficiency, makes it one of the most important algorithms in probabilistic inference.
Loopy BP çš„ç»éªŒæˆåŠŸï¼ŒåŠ ä¸Šå…¶ç®€å•æ€§å’Œé«˜æ•ˆæ€§ï¼Œä½¿å…¶æˆä¸ºæ¦‚ç‡æ¨ç†ä¸­æœ€é‡è¦çš„ç®—æ³•ä¹‹ä¸€ã€‚</p>
<hr />
<h2>9. Numerical Example</h2>
<p>9. æ•°å€¼ç¤ºä¾‹</p>
<p>To make the algorithm concrete, consider a simple chain factor graph with three binary variables:
ä¸ºäº†ä½¿ç®—æ³•å…·ä½“åŒ–ï¼Œè€ƒè™‘ä¸€ä¸ªå…·æœ‰ä¸‰ä¸ªäºŒå…ƒå˜é‡çš„ç®€å•é“¾å› å­å›¾ï¼š</p>
<p>$p(x_1, x_2, x_3) = f_1(x_1, x_2) \cdot f_2(x_2, x_3)$</p>
<p>where each variable takes values in ${0, 1}$, and the factor tables are:
å…¶ä¸­æ¯ä¸ªå˜é‡çš„å–å€¼èŒƒå›´ä¸º ${0, 1}$ ï¼Œå› å­è¡¨å¦‚ä¸‹ï¼š</p>
<p>$f_1(x_1, x_2) = \begin{pmatrix} 0.8 &amp; 0.2 \ 0.3 &amp; 0.7 \end{pmatrix}, \quad f_2(x_2, x_3) = \begin{pmatrix} 0.6 &amp; 0.4 \ 0.1 &amp; 0.9 \end{pmatrix}$</p>
<p>Here, $f_1(x_1=0, x_2=0) = 0.8$, $f_1(x_1=0, x_2=1) = 0.2$, etc.
è¿™é‡Œï¼Œ $f_1(x_1=0, x_2=0) = 0.8$ ï¼Œ $f_1(x_1=0, x_2=1) = 0.2$ ï¼Œç­‰ç­‰ã€‚</p>
<div class="video-container">
  <p class="video-caption">Belief Convergence / ä¿¡å¿µæ”¶æ•›</p>
  <video controls preload="metadata">
    <source src="media/videos/bp_scenes/720p30/BeliefConvergence.mp4" type="video/mp4">
  </video>
  <a class="download-link" href="media/videos/bp_scenes/720p30/BeliefConvergence.mp4" download>â¬‡ ä¸‹è½½è§†é¢‘</a>
</div>
<h3>9.1 Step-by-Step Computation</h3>
<p>9.1 é€æ­¥è®¡ç®—</p>
<p><strong>Initialization</strong>: All messages set to $(1, 1)$ (uniform).
<strong>åˆå§‹åŒ–</strong> ï¼šæ‰€æœ‰æ¶ˆæ¯è®¾ç½®ä¸º $(1, 1)$ ï¼ˆç»Ÿä¸€ï¼‰ã€‚</p>
<p><strong>Forward Pass (left to right):
å‰ä¼ ï¼ˆä»å·¦åˆ°å³ï¼‰ï¼š</strong></p>
<ol>
<li>
<p><strong>Leaf message</strong> $\mu_{x_1 \to f_1}(x_1) = (1, 1)$
<strong>å¶å­æ¶ˆæ¯</strong> $\mu_{x_1 \to f_1}(x_1) = (1, 1)$</p>
</li>
<li>
<p><strong>Factor-to-variable</strong> $\mu_{f_1 \to x_2}(x_2)$:
<strong>å› å­åˆ°å˜é‡</strong> $\mu_{f_1 \to x_2}(x_2)$ ï¼š</p>
</li>
</ol>
<p>$\mu_{f_1 \to x_2}(x_2=0) = \sum_{x_1} f_1(x_1, x_2=0) \cdot \mu_{x_1 \to f_1}(x_1) = 0.8 + 0.3 = 1.1$</p>
<p>$\mu_{f_1 \to x_2}(x_2=1) = \sum_{x_1} f_1(x_1, x_2=1) \cdot \mu_{x_1 \to f_1}(x_1) = 0.2 + 0.7 = 0.9$</p>
<p>After normalization: $\mu_{f_1 \to x_2} = (0.55, 0.45)$
å½’ä¸€åŒ–åï¼š $\mu_{f_1 \to x_2} = (0.55, 0.45)$</p>
<ol start="3">
<li>
<p><strong>Variable-to-factor</strong> $\mu_{x_2 \to f_2} = \mu_{f_1 \to x_2} = (0.55, 0.45)$
<strong>å˜é‡åˆ°å› å­</strong> $\mu_{x_2 \to f_2} = \mu_{f_1 \to x_2} = (0.55, 0.45)$</p>
</li>
<li>
<p><strong>Factor-to-variable</strong> $\mu_{f_2 \to x_3}(x_3)$:
<strong>å› å­åˆ°å˜é‡</strong> $\mu_{f_2 \to x_3}(x_3)$ ï¼š</p>
</li>
</ol>
<p>$\mu_{f_2 \to x_3}(x_3=0) = 0.55 \times 0.6 + 0.45 \times 0.1 = 0.375$</p>
<p>$\mu_{f_2 \to x_3}(x_3=1) = 0.55 \times 0.4 + 0.45 \times 0.9 = 0.625$</p>
<p>After normalization: $\mu_{f_2 \to x_3} = (0.375, 0.625)$
å½’ä¸€åŒ–åï¼š $\mu_{f_2 \to x_3} = (0.375, 0.625)$</p>
<p><strong>Backward Pass (right to left):
åä¼ ï¼ˆä»å³åˆ°å·¦ï¼‰ï¼š</strong></p>
<ol start="5">
<li>
<p><strong>Leaf message</strong> $\mu_{x_3 \to f_2}(x_3) = (1, 1)$
<strong>å¶å­æ¶ˆæ¯</strong> $\mu_{x_3 \to f_2}(x_3) = (1, 1)$</p>
</li>
<li>
<p><strong>Factor-to-variable</strong> $\mu_{f_2 \to x_2}(x_2)$:
<strong>å› å­åˆ°å˜é‡</strong> $\mu_{f_2 \to x_2}(x_2)$ ï¼š</p>
</li>
</ol>
<p>$\mu_{f_2 \to x_2}(x_2=0) = 0.6 + 0.4 = 1.0$</p>
<p>$\mu_{f_2 \to x_2}(x_2=1) = 0.1 + 0.9 = 1.0$</p>
<p>After normalization: $\mu_{f_2 \to x_2} = (0.5, 0.5)$
å½’ä¸€åŒ–åï¼š $\mu_{f_2 \to x_2} = (0.5, 0.5)$</p>
<p><strong>Belief Computation:
ä¿¡å¿µè®¡ç®—ï¼š</strong></p>
<p>$b(x_2) \propto \mu_{f_1 \to x_2} \cdot \mu_{f_2 \to x_2} = (0.55 \times 0.5, 0.45 \times 0.5) = (0.275, 0.225)$</p>
<p>After normalization: $b(x_2) = (0.55, 0.45)$
å½’ä¸€åŒ–åï¼š $b(x_2) = (0.55, 0.45)$</p>
<p>The animation below shows how the belief distributions for all three variables converge over iterations of the BP algorithm.
ä¸‹é¢çš„åŠ¨ç”»å±•ç¤ºäº† BP ç®—æ³•è¿­ä»£è¿‡ç¨‹ä¸­æ‰€æœ‰ä¸‰ä¸ªå˜é‡çš„ç½®ä¿¡åˆ†å¸ƒæ˜¯å¦‚ä½•æ”¶æ•›çš„ã€‚</p>
<hr />
<h2>10. Applications</h2>
<p>10. åº”ç”¨</p>
<p>Belief Propagation and its variants have found widespread use across numerous domains:
ä¿¡å¿µä¼ æ’­åŠå…¶å˜ä½“å·²åœ¨ä¼—å¤šé¢†åŸŸå¾—åˆ°å¹¿æ³›åº”ç”¨ï¼š</p>
<h3>10.1 Error-Correcting Codes</h3>
<p>10.1 çº é”™ç </p>
<p>BP is the decoding algorithm underlying two of the most important classes of error-correcting codes:
BP æ˜¯ä¸¤ç§æœ€é‡è¦çš„çº é”™ç çš„åŸºç¡€è§£ç ç®—æ³•ï¼š</p>
<ul>
<li><strong>Turbo Codes</strong> (Berrou et al., 1993): Use iterative BP-like decoding between two constituent convolutional codes, achieving near-Shannon-limit performance.
<strong>Turbo ç </strong> ï¼ˆBerrou ç­‰äººï¼Œ1993 å¹´ï¼‰ï¼šåœ¨ä¸¤ä¸ªç»„æˆå·ç§¯ç ä¹‹é—´ä½¿ç”¨è¿­ä»£ BP ç±»è§£ç ï¼Œè¾¾åˆ°æ¥è¿‘é¦™å†œæé™çš„æ€§èƒ½ã€‚</li>
<li><strong>Low-Density Parity-Check (LDPC) Codes</strong> (Gallager, 1962; rediscovered by MacKay, 1999): The parity-check matrix defines a factor graph, and BP decoding (also called &quot;sum-product decoding&quot;) achieves remarkable performance. LDPC codes are used in 5G, Wi-Fi (802.11n/ac), and digital television standards.
<strong>ä½å¯†åº¦å¥‡å¶æ ¡éªŒç ï¼ˆLDPCï¼‰</strong> ï¼ˆGallagerï¼Œ1962ï¼›MacKayï¼Œ1999 å¹´é‡æ–°å‘ç°ï¼‰ï¼šå¥‡å¶æ ¡éªŒçŸ©é˜µå®šä¹‰äº†ä¸€ä¸ªå› å­å›¾ï¼ŒBP è¯‘ç ï¼ˆä¹Ÿç§°ä¸ºâ€œå’Œç§¯è¯‘ç â€ï¼‰å¯å®ç°å“è¶Šçš„æ€§èƒ½ã€‚LDPC ç åº”ç”¨äº 5Gã€Wi-Fiï¼ˆ802.11n/acï¼‰å’Œæ•°å­—ç”µè§†æ ‡å‡†ä¸­ã€‚</li>
</ul>
<h3>10.2 Computer Vision</h3>
<p>10.2 è®¡ç®—æœºè§†è§‰</p>
<ul>
<li><strong>Stereo Matching</strong>: BP on MRFs finds pixel-wise depth maps by enforcing smoothness between neighboring pixels while matching left-right image pairs.
<strong>ç«‹ä½“åŒ¹é…</strong> ï¼šBP on MRFs é€šè¿‡å¼ºåˆ¶ç›¸é‚»åƒç´ ä¹‹é—´çš„å¹³æ»‘æ€§æ¥åŒ¹é…å·¦å³å›¾åƒå¯¹ï¼Œä»è€Œæ‰¾åˆ°åƒç´ çº§æ·±åº¦å›¾ã€‚</li>
<li><strong>Image Segmentation</strong>: MRF-based models with BP inference assign semantic labels to image regions.
<strong>å›¾åƒåˆ†å‰²</strong> ï¼šåŸºäº MRF çš„ BP æ¨ç†æ¨¡å‹ä¸ºå›¾åƒåŒºåŸŸåˆ†é…è¯­ä¹‰æ ‡ç­¾ã€‚</li>
<li><strong>Object Detection and Pose Estimation</strong>: Pictorial structure models use BP to efficiently reason about spatial configurations of object parts.
<strong>ç›®æ ‡æ£€æµ‹å’Œå§¿æ€ä¼°è®¡</strong> ï¼šå›¾åƒç»“æ„æ¨¡å‹ä½¿ç”¨ BP æ¥æœ‰æ•ˆåœ°æ¨ç†ç‰©ä½“éƒ¨åˆ†çš„ç©ºé—´é…ç½®ã€‚</li>
<li><strong>Image Denoising</strong>: Removing noise from images by propagating local evidence through an MRF.
<strong>å›¾åƒå»å™ª</strong> ï¼šé€šè¿‡ MRF ä¼ æ’­å±€éƒ¨è¯æ®æ¥å»é™¤å›¾åƒä¸­çš„å™ªå£°ã€‚</li>
</ul>
<h3>10.3 Natural Language Processing</h3>
<p>10.3 è‡ªç„¶è¯­è¨€å¤„ç†</p>
<ul>
<li><strong>Part-of-Speech Tagging</strong>: The forward-backward algorithm (a special case of BP on HMMs) computes marginal tag probabilities.
<strong>è¯æ€§æ ‡æ³¨</strong> ï¼šå‰å‘-åå‘ç®—æ³•ï¼ˆHMM ä¸Šçš„ BP ç®—æ³•çš„ä¸€ä¸ªç‰¹ä¾‹ï¼‰è®¡ç®—è¾¹ç¼˜æ ‡ç­¾æ¦‚ç‡ã€‚</li>
<li><strong>Named Entity Recognition</strong>: CRF models decoded with BP.
<strong>å‘½åå®ä½“è¯†åˆ«</strong> ï¼šä½¿ç”¨ BP è§£ç çš„ CRF æ¨¡å‹ã€‚</li>
<li><strong>Machine Translation</strong>: Alignment models and syntax-based translation use BP-like message passing.
<strong>æœºå™¨ç¿»è¯‘</strong> ï¼šå¯¹é½æ¨¡å‹å’ŒåŸºäºè¯­æ³•çš„ç¿»è¯‘ä½¿ç”¨ç±»ä¼¼ BP çš„æ¶ˆæ¯ä¼ é€’ã€‚</li>
</ul>
<h3>10.4 Computational Biology</h3>
<p>10.4 è®¡ç®—ç”Ÿç‰©å­¦</p>
<ul>
<li><strong>Protein Structure Prediction</strong>: BP on residue interaction networks.
<strong>è›‹ç™½è´¨ç»“æ„é¢„æµ‹</strong> ï¼šåŸºäºæ®‹åŸºç›¸äº’ä½œç”¨ç½‘ç»œçš„ BPã€‚</li>
<li><strong>Gene Regulatory Networks</strong>: Inferring gene expression states.
<strong>åŸºå› è°ƒæ§ç½‘ç»œ</strong> ï¼šæ¨æ–­åŸºå› è¡¨è¾¾çŠ¶æ€ã€‚</li>
<li><strong>Phylogenetics</strong>: Computing likelihoods on evolutionary trees (Felsenstein's pruning algorithm is a special case of BP).
<strong>ç³»ç»Ÿå‘è‚²å­¦</strong> ï¼šè®¡ç®—è¿›åŒ–æ ‘çš„ä¼¼ç„¶æ€§ï¼ˆFelsenstein å‰ªæç®—æ³•æ˜¯ BP çš„ä¸€ä¸ªç‰¹ä¾‹ï¼‰ã€‚</li>
</ul>
<h3>10.5 Robotics and SLAM</h3>
<p>10.5 æœºå™¨äººä¸ SLAM</p>
<ul>
<li><strong>Simultaneous Localization and Mapping (SLAM)</strong>: Factor graph models with BP solve the robot localization problem.
<strong>åŒæ—¶å®šä½ä¸å»ºå›¾ï¼ˆSLAMï¼‰</strong> ï¼šé‡‡ç”¨ BP ç®—æ³•çš„å› å­å›¾æ¨¡å‹è§£å†³æœºå™¨äººå®šä½é—®é¢˜ã€‚</li>
<li><strong>Sensor Fusion</strong>: Combining information from multiple noisy sensors using message passing.
<strong>ä¼ æ„Ÿå™¨èåˆ</strong> ï¼šåˆ©ç”¨æ¶ˆæ¯ä¼ é€’å°†æ¥è‡ªå¤šä¸ªå™ªå£°ä¼ æ„Ÿå™¨çš„ä¿¡æ¯ç»“åˆèµ·æ¥ã€‚</li>
</ul>
<hr />
<h2>11. Conclusion</h2>
<p>11. ç»“è®º</p>
<p>Belief Propagation is a powerful and versatile algorithm for probabilistic inference on graphical models. Its key strengths include:
ä¿¡å¿µä¼ æ’­ç®—æ³•æ˜¯ä¸€ç§åŠŸèƒ½å¼ºå¤§ä¸”ç”¨é€”å¹¿æ³›çš„ç®—æ³•ï¼Œé€‚ç”¨äºå›¾æ¨¡å‹ä¸Šçš„æ¦‚ç‡æ¨ç†ã€‚å…¶ä¸»è¦ä¼˜åŠ¿åŒ…æ‹¬ï¼š</p>
<ol>
<li><strong>Exactness on trees</strong>: The Sum-Product algorithm provably computes exact marginals on tree-structured factor graphs in linear time.
<strong>æ ‘ä¸Šçš„ç²¾ç¡®æ€§</strong> ï¼šSum-Product ç®—æ³•å¯è¯æ˜èƒ½åœ¨çº¿æ€§æ—¶é—´å†…è®¡ç®—æ ‘çŠ¶å› å­å›¾ä¸Šçš„ç²¾ç¡®è¾¹ç¼˜åˆ†å¸ƒã€‚</li>
<li><strong>Practical effectiveness on loopy graphs</strong>: Despite lacking convergence guarantees, Loopy BP provides excellent approximations in many real-world applications.
<strong>åœ¨å¾ªç¯å›¾ä¸Šçš„å®é™…æœ‰æ•ˆæ€§</strong> ï¼šå°½ç®¡ç¼ºä¹æ”¶æ•›æ€§ä¿è¯ï¼Œä½† Loopy BP åœ¨è®¸å¤šå®é™…åº”ç”¨ä¸­æä¾›äº†æä½³çš„è¿‘ä¼¼ç»“æœã€‚</li>
<li><strong>Modularity</strong>: The local message-passing rules are simple and modular â€” the algorithm naturally decomposes global inference into local computations.
<strong>æ¨¡å—åŒ–</strong> ï¼šå±€éƒ¨æ¶ˆæ¯ä¼ é€’è§„åˆ™ç®€å•ä¸”æ¨¡å—åŒ–â€”â€”è¯¥ç®—æ³•è‡ªç„¶åœ°å°†å…¨å±€æ¨ç†åˆ†è§£ä¸ºå±€éƒ¨è®¡ç®—ã€‚</li>
<li><strong>Versatility</strong>: By changing the &quot;sum&quot; to a &quot;max&quot;, the same framework handles both marginal inference (Sum-Product) and MAP inference (Max-Product).
<strong>å¤šåŠŸèƒ½æ€§</strong> ï¼šé€šè¿‡å°†â€œæ±‚å’Œâ€æ”¹ä¸ºâ€œæœ€å¤§å€¼â€ï¼ŒåŒä¸€ä¸ªæ¡†æ¶å¯ä»¥å¤„ç†è¾¹é™…æ¨æ–­ï¼ˆæ±‚å’Œ-ä¹˜ç§¯ï¼‰å’Œæœ€å¤§åéªŒæ¦‚ç‡æ¨æ–­ï¼ˆæœ€å¤§-ä¹˜ç§¯ï¼‰ã€‚</li>
<li><strong>Theoretical depth</strong>: Connections to variational inference, the Bethe free energy, and information geometry provide a rich theoretical understanding.
<strong>ç†è®ºæ·±åº¦</strong> ï¼šä¸å˜åˆ†æ¨æ–­ã€è´ç‰¹è‡ªç”±èƒ½å’Œä¿¡æ¯å‡ ä½•çš„è”ç³»æä¾›äº†ä¸°å¯Œçš„ç†è®ºç†è§£ã€‚</li>
</ol>
<p>Future directions include:
æœªæ¥å‘å±•æ–¹å‘åŒ…æ‹¬ï¼š</p>
<ul>
<li><strong>Neural Belief Propagation</strong>: Combining BP with neural networks for learned message functions
<strong>ç¥ç»ä¿¡å¿µä¼ æ’­</strong> ï¼šå°†åå‘ä¼ æ’­ç®—æ³•ä¸ç¥ç»ç½‘ç»œç›¸ç»“åˆï¼Œç”¨äºå­¦ä¹ æ¶ˆæ¯å‡½æ•°</li>
<li><strong>Generalized BP</strong>: Extensions like Expectation Propagation and Region-based BP that improve approximation quality
<strong>å¹¿ä¹‰ BP</strong> ï¼šè¯¸å¦‚æœŸæœ›ä¼ æ’­å’ŒåŸºäºåŒºåŸŸçš„ BP ç­‰æ‰©å±•æ–¹æ³•å¯ä»¥æé«˜é€¼è¿‘è´¨é‡</li>
<li><strong>Quantum Belief Propagation</strong>: Adapting message passing for quantum probabilistic models
<strong>é‡å­ä¿¡å¿µä¼ æ’­</strong> ï¼šå°†æ¶ˆæ¯ä¼ é€’åº”ç”¨äºé‡å­æ¦‚ç‡æ¨¡å‹</li>
</ul>
<p>Belief Propagation remains one of the most elegant and practically impactful algorithms at the intersection of probability theory, graph theory, and computer science.
ä¿¡å¿µä¼ æ’­ä»ç„¶æ˜¯æ¦‚ç‡è®ºã€å›¾è®ºå’Œè®¡ç®—æœºç§‘å­¦äº¤å‰é¢†åŸŸä¸­æœ€ä¼˜é›…ã€æœ€å…·å®é™…å½±å“åŠ›çš„ç®—æ³•ä¹‹ä¸€ã€‚</p>
<hr />
<h2>12. References</h2>
<p>12. å‚è€ƒæ–‡çŒ®</p>
<ol>
<li>
<p>Pearl, J. (1988). <em>Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</em>. Morgan Kaufmann.
Pearl, J. (1988). <em>æ™ºèƒ½ç³»ç»Ÿä¸­çš„æ¦‚ç‡æ¨ç†ï¼šä¼¼ç„¶æ¨ç†ç½‘ç»œ</em> ã€‚Morgan Kaufmann å‡ºç‰ˆç¤¾ã€‚</p>
</li>
<li>
<p>Kschischang, F. R., Frey, B. J., &amp; Loeliger, H.-A. (2001). Factor graphs and the sum-product algorithm. <em>IEEE Transactions on Information Theory</em>, 47(2), 498â€“519.
Kschischang, FR, Frey, BJ, &amp; Loeliger, H.-A. (2001). å› å­å›¾å’Œæ±‚å’Œä¹˜ç§¯ç®—æ³•ã€‚IEEE <em>ä¿¡æ¯è®ºæ±‡åˆŠ</em> ï¼Œ47(2), 498â€“519ã€‚</p>
</li>
<li>
<p>Yedidia, J. S., Freeman, W. T., &amp; Weiss, Y. (2001). Understanding belief propagation and its generalizations. <em>Exploring Artificial Intelligence in the New Millennium</em>, 8, 236â€“239.
Yedidia, JS, Freeman, WT, &amp; Weiss, Y. (2001). ç†è§£ä¿¡å¿µä¼ æ’­åŠå…¶æ¦‚æ‹¬ã€‚ <em>æ¢ç´¢æ–°åƒå¹´çš„äººå·¥æ™ºèƒ½</em> ï¼Œ8ï¼Œ236â€“239ã€‚</p>
</li>
<li>
<p>Bishop, C. M. (2006). <em>Pattern Recognition and Machine Learning</em>. Springer. (Chapter 8: Graphical Models)
Bishop, CM (2006). <em>æ¨¡å¼è¯†åˆ«ä¸æœºå™¨å­¦ä¹ </em> . Springer. (ç¬¬ 8 ç« ï¼šå›¾å½¢æ¨¡å‹)</p>
</li>
<li>
<p>Koller, D., &amp; Friedman, N. (2009). <em>Probabilistic Graphical Models: Principles and Techniques</em>. MIT Press.
Koller, D. å’Œ Friedman, N. (2009)ã€‚ <em>æ¦‚ç‡å›¾æ¨¡å‹ï¼šåŸç†ä¸æŠ€æœ¯</em> ã€‚éº»çœç†å·¥å­¦é™¢å‡ºç‰ˆç¤¾ã€‚</p>
</li>
<li>
<p>Murphy, K. P. (2012). <em>Machine Learning: A Probabilistic Perspective</em>. MIT Press. (Chapter 20: Exact Inference for Graphical Models; Chapter 22: Variational Inference)
Murphy, KP (2012). <em>æœºå™¨å­¦ä¹ ï¼šæ¦‚ç‡è§†è§’</em> ã€‚éº»çœç†å·¥å­¦é™¢å‡ºç‰ˆç¤¾ã€‚ï¼ˆç¬¬ 20 ç« ï¼šå›¾æ¨¡å‹çš„ç²¾ç¡®æ¨ç†ï¼›ç¬¬ 22 ç« ï¼šå˜åˆ†æ¨ç†ï¼‰</p>
</li>
<li>
<p>Wainwright, M. J., &amp; Jordan, M. I. (2008). Graphical models, exponential families, and variational inference. <em>Foundations and Trends in Machine Learning</em>, 1(1â€“2), 1â€“305.
Wainwright, MJ å’Œ Jordan, MI (2008)ã€‚å›¾å½¢æ¨¡å‹ã€æŒ‡æ•°æ—å’Œå˜åˆ†æ¨æ–­ã€‚ <em>æœºå™¨å­¦ä¹ åŸºç¡€ä¸è¶‹åŠ¿</em> ï¼Œ1(1â€“2)ï¼Œ1â€“305ã€‚</p>
</li>
<li>
<p>Berrou, C., Glavieux, A., &amp; Thitimajshima, P. (1993). Near Shannon limit error-correcting coding and decoding: Turbo-codes. <em>Proceedings of IEEE ICC</em>, 1064â€“1070.
Berrou, C., Glavieux, A., &amp; Thitimajshima, P. (1993). æ¥è¿‘é¦™å†œæé™çš„çº é”™ç¼–ç å’Œè§£ç ï¼šTurbo ç ã€‚IEEE <em>ICC ä¼šè®®è®ºæ–‡é›†</em> ï¼Œ1064â€“1070ã€‚</p>
</li>
<li>
<p>MacKay, D. J. C. (1999). Good error-correcting codes based on very sparse matrices. <em>IEEE Transactions on Information Theory</em>, 45(2), 399â€“431.
MacKay, DJC (1999). åŸºäºç¨€ç–çŸ©é˜µçš„è‰¯å¥½çº é”™ç . <em>IEEE ä¿¡æ¯è®ºæ±‡åˆŠ</em> , 45(2), 399â€“431.</p>
</li>
<li>
<p>Felzenszwalb, P. F., &amp; Huttenlocher, D. P. (2006). Efficient belief propagation for early vision. <em>International Journal of Computer Vision</em>, 70(1), 41â€“54.
Felzenszwalb, PF, &amp; Huttenlocher, DP (2006). æ—©æœŸè§†è§‰çš„æœ‰æ•ˆä¿¡å¿µä¼ æ’­. <em>å›½é™…è®¡ç®—æœºè§†è§‰æ‚å¿—</em> , 70(1), 41â€“54ã€‚</p>
</li>
</ol>

</body>
</html>
